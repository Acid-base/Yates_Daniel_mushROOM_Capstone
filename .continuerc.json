{
  "title": "config.json",
  "$ref": "#/definitions/SerializedContinueConfig",
  "definitions": {
    "BaseCompletionOptions": {
      "title": "BaseCompletionOptions",
      "type": "object",
      "properties": {
        "stream": {
          "title": "Stream",
          "description": "Whether to stream the LLM response. Currently only respected by the 'anthropic' provider. Otherwise will always stream.",
          "type": "boolean",
          "default": true
        },
        "temperature": {
          "title": "Temperature",
          "description": "The temperature of the completion.",
          "type": "number",
          "default": 0.7
        },
        "topP": {
          "title": "Top P",
          "description": "The topP of the completion.",
          "type": "number",
          "default": 0.95
        },
        "topK": {
          "title": "Top K",
          "description": "The topK of the completion.",
          "type": "integer",
          "default": 40
        },
        "presencePenalty": {
          "title": "Presence Penalty",
          "description": "The presence penalty Aof the completion.",
          "type": "number",
          "default": 0
        },
        "frequencePenalty": {
          "title": "Frequency Penalty",
          "description": "The frequency penalty of the completion.",
          "type": "number",
          "default": 0
        },
        "mirostat": {
          "title": "Mirostat",
          "description": "Enable Mirostat sampling, controlling perplexity during text generation (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0). Only available for Ollama, LM Studio, and llama.cpp providers",
          "type": "number",
          "default": 0
        },
        "stop": {
          "title": "Stop",
          "description": "The stop tokens of the completion.",
          "type": "array",
          "items": {
            "type": "string"
          },
          "default": [
            "\n\n"
          ]
        },
        "maxTokens": {
          "title": "Max Tokens",
          "description": "The maximum number of tokens to generate.",
          "default": 600,
          "type": "integer"
        },
        "numThreads": {
          "title": "Number of threads",
          "description": "The number of threads used in the generation process. Only available for Ollama (this is the num_thread parameter)",
          "type": "integer",
          "default": 8
        },
        "keepAlive": {
          "title": "Ollama keep_alive",
          "description": "The number of seconds after no requests are made to unload the model from memory. Defaults to 60*30 = 30min",
          "type": "integer",
          "default": 1800
        }
      }
    },
    "RequestOptions": {
      "title": "RequestOptions",
      "type": "object",
      "properties": {
        "timeout": {
          "title": "Timeout",
          "description": "Set the timeout for each request to the LLM. If you are running a local LLM that takes a while to respond, you might want to set this to avoid timeouts.",
          "default": 7200,
          "type": "integer"
        },
        "verifySsl": {
          "title": "Verify Ssl",
          "description": "Whether to verify SSL certificates for requests.",
          "type": "boolean",
          "default": true
        },
        "caBundlePath": {
          "title": "Ca Bundle Path",
          "description": "Path to a custom CA bundle to use when making the HTTP request",
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "array",
              "items": {
                "type": "string"
              }
            }
          ]
        },
        "proxy": {
          "title": "Proxy",
          "description": "Proxy URL to use when making the HTTP request",
          "type": "string"
        },
        "headers": {
          "title": "Headers",
          "description": "Headers to use when making the HTTP request",
          "type": "object",
          "additionalProperties": {
            "type": "string"
          }
        },
        "extraBodyProperties": {
          "title": "Extra Body Properties",
          "description": "This object will be merged with the body when making the HTTP requests",
          "type": "object"
        },
        "noProxy": {
          "title": "No Proxy",
          "description": "A list of hostnames for which Continue should not use the proxy specified in requestOptions.proxy",
          "type": "array",
          "items": {
            "type": "string"
          }
        }
      }
    },
    "ModelDescription": {
      "title": "ModelDescription",
      "type": "object",
      "properties": {
        "title": {
          "title": "Title",
          "description": "The title you wish to give your model.",
          "type": "string",
          "default": "GPT-3.5 Turbo"
        },
        "provider": {
          "title": "Provider",
          "description": "The provider of the model. This is used to determine the type of model, and how to interact with it.",
          "enum": [
            "openai",
            "free-trial",
            "anthropic",
            "cohere",
            "bedrock",
            "together",
            "ollama",
            "huggingface-tgi",
            "huggingface-inference-api",
            "llama.cpp",
            "replicate",
            "gemini",
            "lmstudio",
            "llamafile",
            "mistral",
            "deepinfra",
            "flowise",
            "groq",
            "fireworks",
            "continue-proxy",
            "cloudflare"
          ],
          "markdownEnumDescriptions": [
            "### OpenAI\nUse gpt-4, gpt-3.5-turbo, or any other OpenAI model. See [here](https://openai.com/product#made-for-developers) to obtain an API key.\n\n> [Reference](https://docs.continue.dev/reference/Model%20Providers/openai)",
            "### Free Trial\nNew users can try out Continue for free using a proxy server that securely makes calls to OpenAI using our API key. If you are ready to use your own API key or have used all 250 free uses, you can enter your API key in config.json where it says `apiKey=\"\"` or select another model provider.\n> [Reference](https://docs.continue.dev/reference/Model%20Providers/freetrial)",
            "### Anthropic\nTo get started with Anthropic models, you first need to sign up for the open beta [here](https://claude.ai/login) to obtain an API key.\n> [Reference](https://docs.continue.dev/reference/Model%20Providers/anthropicllm)",
            "### Cohere\nTo use Cohere, visit the [Cohere dashboard](https://dashboard.cohere.com/api-keys) to create an API key.\n\n> [Reference](https://docs.continue.dev/reference/Model%20Providers/cohere)",
            "### Bedrock\nTo get started with Bedrock you need to sign up on AWS [here](https://aws.amazon.com/bedrock/claude/)",
            "### Together\nTogether is a hosted service that provides extremely fast streaming of open-source language models. To get started with Together:\n1. Obtain an API key from [here](https://together.ai)\n2. Paste below\n3. Select a model preset\n> [Reference](https://docs.continue.dev/reference/Model%20Providers/togetherllm)",
            "### Ollama\nTo get started with Ollama, follow these steps:\n1. Download from [ollama.ai](https://ollama.ai/) and open the application\n2. Open a terminal and run `ollama run <MODEL_NAME>`. Example model names are `codellama:7b-instruct` or `llama2:7b-text`. You can find the full list [here](https://ollama.ai/library).\n3. Make sure that the model name used in step 2 is the same as the one in config.json (e.g. `model=\"codellama:7b-instruct\"`)\n4. Once the model has finished downloading, you can start asking questions through Continue.\n> [Reference](https://docs.continue.dev/reference/Model%20Providers/ollama)",
            "### Huggingface TGI\n\n> [Reference](https://docs.continue.dev/reference/Model%20Providers/huggingfacetgi)",
            "### Huggingface Inference API\n\n> [Reference](https://docs.continue.dev/reference/Model%20Providers/huggingfaceinferenceapi)",
            "### Llama.cpp\nllama.cpp comes with a [built-in server](https://github.com/ggerganov/llama.cpp/tree/master/examples/server#llamacppexampleserver) that can be run from source. To do this:\n\n1. Clone the repository with `git clone https://github.com/ggerganov/llama.cpp`.\n2. `cd llama.cpp`\n3. Run `make` to build the server.\n4. Download the model you'd like to use and place it in the `llama.cpp/models` directory (the best place to find models is [The Bloke on HuggingFace](https://huggingface.co/TheBloke))\n5. Run the llama.cpp server with the command below (replacing with the model you downloaded):\n\n```shell\n.\\server.exe -c 4096 --host 0.0.0.0 -t 16 --mlock -m models/codellama-7b-instruct.Q8_0.gguf\n```\n\nAfter it's up and running, you can start using Continue.\n> [Reference](https://docs.continue.dev/reference/Model%20Providers/llamacpp)",
            "### Replicate\nReplicate is a hosted service that makes it easy to run ML models. To get started with Replicate:\n1. Obtain an API key from [here](https://replicate.com)\n2. Paste below\n3. Select a model preset\n> [Reference](https://docs.continue.dev/reference/Model%20Providers/replicatellm)",
            "### Gemini API\nTo get started with Google Makersuite, obtain your API key from [here](https://makersuite.google.com) and paste it below.\n> [Reference](https://docs.continue.dev/reference/Model%20Providers/geminiapi)",
            "### LMStudio\nLMStudio provides a professional and well-designed GUI for exploring, configuring, and serving LLMs. It is available on both Mac and Windows. To get started:\n1. Download from [lmstudio.ai](https://lmstudio.ai/) and open the application\n2. Search for and download the desired model from the home screen of LMStudio.\n3. In the left-bar, click the '<->' icon to open the Local Inference Server and press 'Start Server'.\n4. Once your model is loaded and the server has started, you can begin using Continue.\n> [Reference](https://docs.continue.dev/reference/Model%20Providers/lmstudio)",
            "### Llamafile\nTo get started with llamafiles, find and download a binary on their [GitHub repo](https://github.com/Mozilla-Ocho/llamafile#binary-instructions). Then run it with the following command:\n\n```shell\nchmod +x ./llamafile\n./llamafile\n```\n> [Reference](https://docs.continue.dev/reference/Model%20Providers/llamafile)",
            "### Mistral API\n\nTo get access to the Mistral API, obtain your API key from the [Mistral platform](https://docs.mistral.ai/)",
            "### DeepInfra\n\n> [Reference](https://docs.continue.dev/reference/Model%20Providers/deepinfra)",
            "### Continue Proxy\n\nContinue Enterprise users can use the Continue Proxy as a single point of access to models.",
            "### Cloudflare Workers AI\n\n[Reference](https://developers.cloudflare.com/workers-ai/)"
          ],
          "type": "string",
          "default": "openai"
        },
        "model": {
          "title": "Model",
          "description": "The name of the model. Used to autodetect prompt template.",
          "type": "string",
          "default": "gpt-3.5-turbo"
        },
        "apiKey": {
          "title": "Api Key",
          "description": "OpenAI, Anthropic, Cohere, Together, or other API key",
          "type": "string",
          "default": "YOUR_OPENAI_API_KEY"
        },
        "apiBase": {
          "title": "Api Base",
          "description": "The base URL of the LLM API.",
          "type": "string"
        },
        "contextLength": {
          "title": "Context Length",
          "description": "The maximum context length of the LLM in tokens, as counted by countTokens.",
          "default": 2048,
          "type": "integer"
        },
        "template": {
          "title": "Template",
          "description": "The chat template used to format messages. This is auto-detected for most models, but can be overridden here. Choose none if you are using vLLM or another server that automatically handles prompting.",
          "enum": [
            "llama2",
            "alpaca",
            "zephyr",
            "phi2",
            "phind",
            "anthropic",
            "chatml",
            "none",
            "deepseek",
            "openchat",
            "xwin-coder",
            "neural-chat",
            "codellama-70b",
            "llava",
            "gemma",
            "llama3"
          ],
          "type": "string",
          "default": "llama2"
        },
        "promptTemplates": {
          "title": "Prompt Templates",
          "markdownDescription": "A mapping of prompt template name ('edit' is currently the only one used in Continue) to a string giving the prompt template. See [here](https://docs.continue.dev/model-setup/configuration#customizing-the-edit-prompt) for an example.",
          "type": "object",
          "additionalProperties": {
            "type": "string"
          }
        },
        "completionOptions": {
          "title": "Completion Options",
          "description": "Options for the completion endpoint. Read more about the completion options in the documentation.",
          "default": {
            "temperature": null,
            "topP": null,
            "topK": null,
            "presencePenalty": null,
            "frequencyPenalty": null,
            "stop": null,
            "maxTokens": 600
          },
          "allOf": [
            {
              "$ref": "#/definitions/BaseCompletionOptions"
            }
          ]
        },
        "systemMessage": {
          "title": "System Message",
          "description": "A system message that will always be followed by the LLM",
          "type": "string",
          "default": "Prioritize solutions using functional, then declarative styles before others.  Pure functions should be the preferred solution when possible. Prototypal objects are acceptable but try to avoid OOP."
        },
        "requestOptions": {
          "title": "Request Options",
          "description": "Options for the HTTP request to the LLM.",
          "default": {
            "timeout": 7200,
            "verifySsl": null,
            "caBundlePath": null,
            "proxy": null,
            "headers": null,
            "extraBodyProperties": null
          },
          "allOf": [
            {
              "$ref": "#/definitions/RequestOptions"
            }
          ]
        },
        "apiType": {
          "title": "Api Type",
          "markdownDescription": "OpenAI API type, either `openai` or `azure`",
          "enum": [
            "openai",
            "azure"
          ],
          "default": "openai"
        },
        "apiVersion": {
          "title": "Api Version",
          "description": "Azure OpenAI API version (e.g. 2023-07-01-preview)",
          "type": "string"
        },
        "engine": {
          "title": "Engine",
          "description": "Azure OpenAI engine",
          "type": "string"
        }
      },
      "required": [
        "title",
        "provider",
        "model"
      ],
      "allOf": [
        {
          "if": {
            "properties": {
              "provider": {
                "type": "str"
              }
            },
            "not": {
              "required": [
                "provider"
              ]
            }
          },
          "then": {
            "properties": {
              "model": {
                "description": "Choose a provider first, then model options will be shown here."
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "openai",
                  "anthropic",
                  "cohere",
                  "gemini",
                  "huggingface-inference-api",
                  "replicate",
                  "together",
                  "cloudflare"
                ]
              }
            },
            "required": [
              "provider"
            ]
          },
          "then": {
            "required": [
              "apiKey"
            ]
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "huggingface-tgi",
                  "huggingface-inference-api"
                ]
              }
            }
          },
          "then": {
            "required": [
              "apiBase"
            ]
          },
          "required": [
            "provider"
          ]
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "openai"
                ]
              }
            },
            "required": [
              "provider"
            ]
          },
          "then": {
            "properties": {
              "engine": {
                "type": "string"
              },
              "apiType": {
                "type": "string"
              },
              "apiVersion": {
                "type": "string"
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "cloudflare"
                ]
              }
            },
            "required": [
              "provider"
            ]
          },
          "then": {
            "properties": {
              "accountId": {
                "type": "string"
              },
              "aiGatewaySlug": {
                "type": "string"
              },
              "model": {
                "anyOf": [
                  {
                    "enum": [
                      "@cf/meta/llama-3-8b-instruct",
                      "@hf/thebloke/deepseek-coder-6.7b-instruct-awq",
                      "@cf/deepseek-ai/deepseek-math-7b-instruct",
                      "@cf/thebloke/discolm-german-7b-v1-awq",
                      "@cf/tiiuae/falcon-7b-instruct",
                      "@cf/google/gemma-2b-it-lora",
                      "@hf/google/gemma-7b-it",
                      "@cf/google/gemma-7b-it-lora",
                      "@hf/nousresearch/hermes-2-pro-mistral-7b",
                      "@cf/meta/llama-2-7b-chat-fp16",
                      "@cf/meta/llama-2-7b-chat-int8",
                      "@cf/meta-llama/llama-2-7b-chat-hf-lora",
                      "@hf/thebloke/llama-2-13b-chat-awq",
                      "@hf/thebloke/llamaguard-7b-awq",
                      "@cf/mistral/mistral-7b-instruct-v0.1",
                      "@hf/mistral/mistral-7b-instruct-v0.2",
                      "@cf/mistral/mistral-7b-instruct-v0.2-lora",
                      "@hf/thebloke/neural-chat-7b-v3-1-awq",
                      "@cf/openchat/openchat-3.5-0106",
                      "@hf/thebloke/openhermes-2.5-mistral-7b-awq",
                      "@cf/microsoft/phi-2",
                      "@cf/qwen/qwen1.5-0.5b-chat",
                      "@cf/qwen/qwen1.5-1.8b-chat",
                      "@cf/qwen/qwen1.5-7b-chat-awq",
                      "@cf/qwen/qwen1.5-14b-chat-awq",
                      "@cf/defog/sqlcoder-7b-2",
                      "@hf/nexusflow/starling-lm-7b-beta",
                      "@cf/tinyllama/tinyllama-1.1b-chat-v1.0",
                      "@hf/thebloke/zephyr-7b-beta-awq",
                      "@hf/thebloke/deepseek-coder-6.7b-base-awq"
                    ]
                  },
                  {
                    "type": "string"
                  }
                ]
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "openai"
                ]
              }
            },
            "required": [
              "provider"
            ]
          },
          "then": {
            "properties": {
              "model": {
                "anyOf": [
                  {
                    "enum": [
                      "gpt-3.5-turbo",
                      "gpt-3.5-turbo-16k",
                      "gpt-4o",
                      "gpt-4",
                      "gpt-3.5-turbo-0613",
                      "gpt-4-32k",
                      "gpt-4-0125-preview",
                      "gpt-4-turbo",
                      "AUTODETECT"
                    ]
                  },
                  {
                    "type": "string"
                  }
                ]
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "replicate"
                ]
              }
            },
            "required": [
              "provider"
            ]
          },
          "then": {
            "properties": {
              "model": {
                "anyOf": [
                  {
                    "enum": [
                      "llama3-8b",
                      "llama3-70b",
                      "codellama-7b",
                      "codellama-13b",
                      "codellama-34b",
                      "llama2-7b",
                      "llama2-13b"
                    ]
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "markdownDescription": "Select a pre-defined option, or find an exact model ID from Replicate [here](https://replicate.com/collections/streaming-language-models)."
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "free-trial"
                ]
              }
            },
            "required": [
              "provider"
            ]
          },
          "then": {
            "properties": {
              "model": {
                "enum": [
                  "gpt-4o",
                  "codestral-latest",
                  "llama3-70b",
                  "gpt-3.5-turbo",
                  "phind-codellama-34b",
                  "gemini-pro",
                  "mistral-8x7b",
                  "claude-3-sonnet-20240229",
                  "claude-3-haiku-20240307",
                  "AUTODETECT"
                ]
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "openai"
                ]
              },
              "apiType": {
                "not": {
                  "const": "azure"
                }
              }
            },
            "required": [
              "provider"
            ]
          },
          "then": {
            "properties": {
              "model": {
                "anyOf": [
                  {
                    "enum": [
                      "gpt-3.5-turbo",
                      "gpt-3.5-turbo-16k",
                      "gpt-4o",
                      "gpt-4",
                      "gpt-3.5-turbo-0613",
                      "gpt-4-32k",
                      "gpt-4-turbo",
                      "gpt-4-vision-preview",
                      "mistral-7b",
                      "mistral-8x7b",
                      "llama2-7b",
                      "llama2-13b",
                      "codellama-7b",
                      "codellama-13b",
                      "codellama-34b",
                      "codellama-70b",
                      "llama3-8b",
                      "llama3-70b",
                      "phind-codellama-34b",
                      "wizardcoder-7b",
                      "wizardcoder-13b",
                      "wizardcoder-34b",
                      "zephyr-7b",
                      "codeup-13b",
                      "deepseek-7b",
                      "deepseek-33b",
                      "neural-chat-7b",
                      "deepseek-1b",
                      "stable-code-3b",
                      "starcoder-1b",
                      "starcoder-3b",
                      "starcoder2-3b",
                      "mistral-tiny",
                      "mistral-small",
                      "mistral-medium",
                      "AUTODETECT"
                    ]
                  },
                  {
                    "type": "string"
                  }
                ]
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "anthropic"
                ]
              }
            },
            "required": [
              "provider"
            ]
          },
          "then": {
            "properties": {
              "model": {
                "anyOf": [
                  {
                    "enum": [
                      "claude-2",
                      "claude-instant-1",
                      "claude-3-opus-20240229",
                      "claude-3-sonnet-20240229",
                      "claude-3-haiku-20240307",
                      "claude-2.1"
                    ]
                  },
                  {
                    "type": "string"
                  }
                ]
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "cohere"
                ]
              }
            },
            "required": [
              "provider"
            ]
          },
          "then": {
            "properties": {
              "model": {
                "enum": [
                  "command-r",
                  "command-r-plus"
                ]
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "bedrock"
                ]
              }
            },
            "required": [
              "provider"
            ]
          },
          "then": {
            "properties": {
              "model": {
                "anyOf": [
                  {
                    "enum": [
                      "claude-3-sonnet-20240229",
                      "claude-3-haiku-20240307",
                      "claude-2"
                    ]
                  },
                  {
                    "type": "string"
                  }
                ]
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "gemini"
                ]
              }
            },
            "required": [
              "provider"
            ]
          },
          "then": {
            "properties": {
              "model": {
                "enum": [
                  "chat-bison-001",
                  "gemini-pro",
                  "gemini-1.5-pro-latest",
                  "gemini-1.5-flash-latest"
                ]
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "together"
                ]
              }
            },
            "required": [
              "provider"
            ]
          },
          "then": {
            "properties": {
              "model": {
                "anyOf": [
                  {
                    "enum": [
                      "mistral-7b",
                      "mistral-8x7b",
                      "llama2-7b",
                      "llama2-13b",
                      "llama3-8b",
                      "llama3-70b",
                      "codellama-7b",
                      "codellama-13b",
                      "codellama-34b",
                      "codellama-70b",
                      "phind-codellama-34b"
                    ]
                  },
                  {
                    "type": "string"
                  }
                ],
                "markdownDescription": "Select a pre-defined option, or find an exact model string from Together AI [here](https://docs.together.ai/docs/inference-models)."
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "deepinfra"
                ]
              }
            },
            "required": [
              "provider"
            ]
          },
          "then": {
            "properties": {
              "model": {
                "markdownDescription": "Find the model name you want to use from DeepInfra [here](https://deepinfra.com/models?type=text-generation)."
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "huggingface-tgi",
                  "huggingface-inference-api",
                  "llama.cpp",
                  "text-gen-webui",
                  "llamafile"
                ]
              }
            },
            "required": [
              "provider"
            ]
          },
          "then": {
            "properties": {
              "model": {
                "anyOf": [
                  {
                    "enum": [
                      "mistral-7b",
                      "mistral-8x7b",
                      "llama2-7b",
                      "llama2-13b",
                      "codellama-7b",
                      "codellama-13b",
                      "codellama-34b",
                      "codellama-70b",
                      "llama3-8b",
                      "llama3-70b",
                      "phind-codellama-34b",
                      "wizardcoder-7b",
                      "wizardcoder-13b",
                      "wizardcoder-34b",
                      "zephyr-7b",
                      "codeup-13b",
                      "deepseek-7b",
                      "deepseek-33b",
                      "neural-chat-7b",
                      "deepseek-1b",
                      "stable-code-3b",
                      "starcoder-1b",
                      "starcoder-3b",
                      "starcoder2-3b"
                    ]
                  },
                  {
                    "type": "string"
                  }
                ]
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "ollama"
                ]
              }
            },
            "required": [
              "provider"
            ]
          },
          "then": {
            "properties": {
              "model": {
                "anyOf": [
                  {
                    "enum": [
                      "mistral-7b",
                      "llama2-7b",
                      "llama2-13b",
                      "codellama-7b",
                      "codellama-13b",
                      "codellama-34b",
                      "codellama-70b",
                      "llama3-8b",
                      "llama3-70b",
                      "phi-2",
                      "phind-codellama-34b"
                    ]
                  }
                ]
              }
            }
          }
        }
      ]
    }
  }
}