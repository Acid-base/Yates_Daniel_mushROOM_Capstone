import pandas as pd
import numpy as np
import logging
from typing import Callable, Dict
import re
import os
import yaml
from pymongo import MongoClient
from pymongo.errors import BulkWriteError

def load_config(config_file: str) -> dict:
    """
    Load configuration from a YAML file.

    Args:
        config_file (str): Path to the YAML configuration file.

    Returns:
        dict: Loaded configuration.
    """
    with open(config_file, 'r') as f:
        return yaml.safe_load(f)

# Load configuration
config = load_config('config.yaml')

# Set up logging
logging.basicConfig(filename=config['log_file'], level=config['log_level'],
                    format='%(asctime)s - %(levelname)s - %(message)s')

def clean_text(text: str) -> str:
    """Remove HTML tags from text and strip whitespace."""
    return re.sub('<[^<]+?>', '', str(text)).strip()

def clean_dataframe(df: pd.DataFrame, column_cleaners: Dict[str, Callable]) -> pd.DataFrame:
    """
    Clean a DataFrame by applying specified cleaning functions to columns.

    Args:
        df (pd.DataFrame): The input DataFrame to clean.
        column_cleaners (Dict[str, Callable]): A dictionary mapping column names to cleaning functions.

    Returns:
        pd.DataFrame: The cleaned DataFrame.
    """
    return df.assign(**{col: cleaner(df[col]) for col, cleaner in column_cleaners.items() if col in df.columns})

def clean_names(df: pd.DataFrame) -> pd.DataFrame:
    return clean_dataframe(df, {
        'text_name': clean_text,
        'author': lambda x: x.str.strip(),
        'deprecated': lambda x: x.astype(bool)
    })

def clean_observations(df: pd.DataFrame) -> pd.DataFrame:
    logging.info("Data types before conversion:")
    logging.info(df.dtypes.to_string())
    logging.info("\nSample 'when' column values before conversion:")
    logging.info(df['when'].head(10).to_string())

    try:
        df['when'] = pd.to_datetime(df['when'], errors='coerce')
        logging.info("\nSample 'when' column values after conversion:")
        logging.info(df['when'].head(10).to_string())
    except Exception as e:
        logging.error(f"Error converting 'when' column: {e}")
        logging.error(df['when'].to_string())
        raise

    return clean_dataframe(df, {
        'notes': clean_text,
        'lat': lambda x: x.fillna(0),
        'lng': lambda x: x.fillna(0),
        'alt': lambda x: x.fillna(0)
    })

def clean_images(df: pd.DataFrame) -> pd.DataFrame:
    license_mapping = {
        "cc-by-sa-3.0": "CC BY-SA 3.0",
        "cc-by-nc-sa-2.0": "CC BY-NC-SA 2.0",
        "cc-by-nc-nd-2.0": "CC BY-NC-ND 2.0",
        "cc-by-nc-sa-2.5": "CC BY-NC-SA 2.5",
        "cc-by-sa-2.5": "CC BY-SA 2.5",
        "cc-by-nc-3.0": "CC BY-NC 3.0",
        "cc-by-2.0": "CC BY 2.0",
        "cc-by-sa-2.0": "CC BY-SA 2.0",
        "cc-by-nc-nd-3.0": "CC BY-NC-ND 3.0",
        "cc-by-nc-sa-3.0": "CC BY-NC-SA 3.0",
        "cc-by-nd-3.0": "CC BY-ND 3.0",
        "cc-by-nc-nd-2.5": "CC BY-NC-ND 2.5",
        "cc-by-sa-4.0": "CC BY-SA 4.0",
        "cc-by-nc-4.0": "CC BY-NC-SA 4.0",
        "cc-by-4.0": "CC BY 4.0",
        "cc0": "CC0 1.0 Universal",
    }
    return clean_dataframe(df, {
        'copyright_holder': lambda x: x.str.strip(),
        'license': lambda x: x.str.lower().map(license_mapping).fillna(x)
    })

def clean_location_descriptions(df: pd.DataFrame) -> pd.DataFrame:
    return clean_dataframe(df, {col: clean_text for col in ['gen_desc', 'ecology', 'species', 'notes', 'refs']})

def clean_locations(df: pd.DataFrame) -> pd.DataFrame:
    return clean_dataframe(df, {
        'name': clean_text,
        'city': lambda x: x.str.strip().str.lower(),
        'state': lambda x: x.str.strip().str.lower(),
        'country': lambda x: x.str.strip().str.lower()
    })

def clean_name_descriptions(df: pd.DataFrame) -> pd.DataFrame:
    return clean_dataframe(df, {'description': clean_text})

def load_and_clean_csv(filename: str, cleaning_func: Callable[[pd.DataFrame], pd.DataFrame], converters: Dict[str, Callable]) -> pd.DataFrame:
    """
    Load a CSV file, apply converters, and clean the resulting DataFrame.

    Args:
        filename (str): The name of the CSV file to load.
        cleaning_func (Callable[[pd.DataFrame], pd.DataFrame]): A function to clean the loaded DataFrame.
        converters (Dict[str, Callable]): A dictionary of functions to convert values in certain columns.

    Returns:
        pd.DataFrame: The loaded, converted, and cleaned DataFrame.

    Raises:
        Exception: If there's an error processing the file.
    """
    try:
        df = pd.read_csv(filename, sep="\t", header=0, low_memory=False, 
                         converters=converters, na_values=['NULL'])
        cleaned_df = cleaning_func(df)
        log_data_types(cleaned_df, filename)
        return cleaned_df
    except Exception as e:
        logging.error(f"Error processing {filename}: {e}")
        return pd.DataFrame()

def log_data_types(df: pd.DataFrame, filename: str) -> None:
    logging.info(f"Data types for {filename}:")
    logging.info(df.dtypes.to_string())
    logging.info("\n")

def save_dataframe(df: pd.DataFrame, filename: str, format: str = 'csv') -> None:
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    if format == 'csv':
        df.to_csv(filename, sep="\t", index=False)
    elif format == 'json':
        df.to_json(filename, orient='records', lines=True)

def safe_float_convert(x):
    try:
        return float(x)
    except ValueError:
        return np.nan

converters = {
    "names.csv": {
        'id': int,
        'text_name': str,
        'author': str,
        'deprecated': lambda x: x.lower() == 'true',
        'correct_spelling_id': safe_float_convert,
        'synonym_id': safe_float_convert,
        'rank': int
    },
    "observations.csv": {
        'id': int,
        'name_id': int,
        'when': str,
        'is_collection_location': int,
        'thumb_image_id': safe_float_convert
    },
    "images.csv": {
        'id': int,
        'content_type': str,
        'copyright_holder': str,
        'license': str,
        'ok_for_export': int,
        'diagnostic': int
    },
    "location_descriptions.csv": {
        'id': str,
        'location_id': str,
        'source_type': str,
        'source_name': str,
        'gen_desc': str,
        'ecology': str,
        'species': str,
        'notes': str,
        'refs': str
    },
    "locations.csv": {
        'id': int,
        'name': str,
        'north': safe_float_convert,
        'south': safe_float_convert,
        'east': safe_float_convert,
        'west': safe_float_convert,
        'high': safe_float_convert,
        'low': safe_float_convert
    },
    "images_observations.csv": {
        'image_id': int,
        'observation_id': int
    },
    "name_classifications.csv": {
        'name_id': int,
        'domain': str,
        'kingdom': str,
        'phylum': str,
        'class': str,
        'order': str,
        'family': str
    },
    "name_descriptions.csv": {
        'id': str,
        'name_id': str,
        'source_type': str,
        'source_name': str,
        'general_description': str,
        'diagnostic_description': str,
        'distribution': str,
        'habitat': str,
        'look_alikes': str,
        'uses': str,
        'notes': str,
        'refs': str
    }
}

def validate_dataframe(df: pd.DataFrame, expected_columns: list) -> bool:
    missing_columns = set(expected_columns) - set(df.columns)
    if missing_columns:
        logging.warning(f"Missing columns: {missing_columns}")
        return False
    return True

def transform_for_upsert(df: pd.DataFrame) -> pd.DataFrame:
    return df.dropna()

def diff_dataframes(df1: pd.DataFrame, df2: pd.DataFrame, filename: str, log_file: str) -> None:
    logging.info(f"Comparing dataframes for {filename}")

    df1 = df1.reset_index(drop=True)
    df2 = df2.reset_index(drop=True)
    df1, df2 = df1.align(df2, join='outer', axis=None)

    try:
        diff = df1.compare(df2)
        if not diff.empty:
            logging.info(f"Differences found in {filename}:", extra={'log_file': log_file})
            for col in diff.columns.get_level_values(0).unique():
                col_diff = diff[col]
                if not col_diff.empty:
                    logging.info(f"Column: {col}", extra={'log_file': log_file})
                    logging.info(col_diff.head(10).to_string(), extra={'log_file': log_file})
        else:
            logging.info(f"No differences found in {filename}", extra={'log_file': log_file})

    except ValueError as e:
        logging.error(f"Error comparing dataframes for {filename}: {str(e)}")
        logging.info("Falling back to manual comparison")

        diff = (df1 != df2) | (df1.isnull() ^ df2.isnull())
        if diff.any().any():
            logging.info(f"Differences found in {filename}:", extra={'log_file': log_file})
            for col in diff.columns:
                col_diff = diff[col]
                if col_diff.any():
                    logging.info(f"Column: {col}", extra={'log_file': log_file})
                    logging.info(col_diff.head(10).to_string(), extra={'log_file': log_file})
        else:
            logging.info(f"No differences found in {filename}", extra={'log_file': log_file})

def connect_to_mongodb(config: dict) -> MongoClient:
    """
    Connect to MongoDB using the provided configuration.

    Args:
        config (dict): Configuration dictionary containing database connection details.

    Returns:
        MongoClient: MongoDB client connected to the specified database.
    """
    client = MongoClient(config['database']['host'], config['database']['port'])
    return client[config['database']['name']]

def upsert_to_mongodb(collection, data: list, key_field: str):
    """
    Perform an upsert operation on the specified MongoDB collection.

    Args:
        collection: MongoDB collection object.
        data (list): List of dictionaries containing the data to upsert.
        key_field (str): The field to use as the unique identifier for upsert operations.

    Raises:
        BulkWriteError: If there's an error during the bulk write operation.
    """
    try:
        operations = [
            {
                'update_one': {
                    'filter': {key_field: item[key_field]},
                    'update': {'$set': item},
                    'upsert': True
                }
            }
            for item in data
        ]
        result = collection.bulk_write(operations, ordered=False)
        logging.info(f"Upserted {result.upserted_count} documents, modified {result.modified_count} documents")
    except BulkWriteError as bwe:
        logging.error(f"Bulk write error: {bwe.details}")
        raise

def preprocess_all_csv_files():
    file_cleaners = {
        "names.csv": clean_names,
        "observations.csv": clean_observations,
        "images.csv": clean_images,
        "location_descriptions.csv": clean_location_descriptions,
        "locations.csv": clean_locations,
        "images_observations.csv": lambda df: df,
        "name_classifications.csv": lambda df: df,
        "name_descriptions.csv": clean_name_descriptions
    }

    cleaned_dfs = {}
    for name, cleaner in file_cleaners.items():
        retries = 3
        while retries > 0:
            try:
                df = load_and_clean_csv(
                    os.path.join(config['input_directory'], name),
                    cleaner,
                    converters.get(name, {})
                )
                if not df.empty:
                    intermediate_filename = os.path.join(config['intermediate_directory'], name)
                    save_dataframe(df, intermediate_filename)
                    cleaned_dfs[name] = df
                break
            except Exception as e:
                logging.error(f"Error processing {name}, retries left: {retries}", exc_info=True)
                retries -= 1
                if retries == 0:
                    logging.error(f"Failed to process {name} after 3 attempts")

    logging.info("Phase 1: Intermediate CSVs saved")

    db = connect_to_mongodb(config)

    for name, df in cleaned_dfs.items():
        file_config = next((f for f in config['files'].values() if f['input'] == name), None)
        if file_config and validate_dataframe(df, file_config['columns']):
            transformed_df = transform_for_upsert(df)
            final_filename = os.path.join(config['output_directory'], file_config['output'])
            save_dataframe(transformed_df, final_filename, format='json')
            diff_dataframes(df, transformed_df, name, config['log_file'])

            # Upsert to MongoDB
            collection_name = name.replace('.csv', '')
            collection = db[collection_name]
            upsert_to_mongodb(collection, transformed_df.
